# üßó Desaf√≠os T√©cnicos y Consideraciones

Este documento detalla los principales desaf√≠os t√©cnicos encontrados durante la adaptaci√≥n de Conversationally para ejecuci√≥n local y las soluciones implementadas.

## 1. üß† Modelos Pesados en Local

### Desaf√≠o
Los modelos de IA originales estaban dise√±ados para ejecutarse en la nube con hardware potente:

- **Mistral 7B**: Requiere ~8 GB RAM en versi√≥n cuantizada
- **BETO fine-tuned**: No disponible p√∫blicamente, requiere entrenamiento propio
- **Sentence Transformers**: Relativamente ligero pero requiere descarga inicial

### Soluciones Implementadas

#### Mistral 7B
```python
# Usamos llama-cpp-python para modelos GGUF cuantizados
from llama_cpp import Llama

self.model = Llama(
    model_path=model_path,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=0  # CPU-only por defecto
)
```

**Ventajas:**
- Reducci√≥n del uso de memoria con cuantizaci√≥n Q4_K_M (~4.7 GB)
- Soporte para CPU sin requerir GPU
- Inferencia razonablemente r√°pida

#### Modelo GEC
```python
# Usamos BETO base como fallback
from transformers import AutoModelForTokenClassification

self.model = AutoModelForTokenClassification.from_pretrained(
    "dccuchile/bert-base-spanish-wwm-cased"
)
```

**Limitaciones:**
- No est√° fine-tuned para correcci√≥n gramatical espec√≠fica
- Requiere entrenamiento adicional con dataset COWS-L2H

## 2. ‚ö° Orquestaci√≥n de Modelos en Tiempo Real

### Desaf√≠o
Coordinar tres modelos diferentes sin afectar la experiencia del usuario:

- Latencia acumulada puede superar los 10 segundos
- Manejo concurrente de m√∫ltiples usuarios
- Uso eficiente de memoria

### Soluciones

#### Carga Lazy y Singleton
```python
# Modelos cargados una sola vez al iniciar
class ModelManager:
    _instances = {}
    
    @classmethod
    def get_model(cls, model_type):
        if model_type not in cls._instances:
            cls._instances[model_type] = cls._load_model(model_type)
        return cls._instances[model_type]
```

#### Async/Await para I/O
```python
@app.post("/chat")
async def chat(request: ChatRequest):
    # Procesamiento paralelo donde sea posible
    similarity_task = asyncio.create_task(check_similarity(user_message))
    errors_task = asyncio.create_task(detect_errors(user_message))
    
    is_on_topic = await similarity_task
    grammar_errors = await errors_task
```

#### Cach√© de Respuestas
```python
# Cache LRU para respuestas comunes
from functools import lru_cache

@lru_cache(maxsize=100)
def generate_scenario_response(scenario: str, user_input_hash: str):
    # Generar respuesta solo si no est√° en cach√©
    pass
```

## 3. üíæ Mantenimiento de Estado Conversacional

### Desaf√≠o
El sistema necesita mantener contexto de la conversaci√≥n:

- Historial de mensajes para contexto
- Estado del escenario seleccionado
- Progreso del usuario

### Implementaci√≥n

#### Estructura de Mensajes
```python
class ConversationState:
    def __init__(self):
        self.messages = []
        self.scenario = None
        self.expected_topic = None
        self.error_count = 0
    
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        # Mantener solo √∫ltimos 20 mensajes para limitar contexto
        if len(self.messages) > 20:
            self.messages = self.messages[-20:]
```

#### Manejo de Sesiones
```python
# Para futura implementaci√≥n multi-usuario
session_store = {}

@app.post("/chat")
async def chat(request: ChatRequest, session_id: str = None):
    if session_id and session_id in session_store:
        state = session_store[session_id]
    else:
        state = ConversationState()
        session_id = str(uuid.uuid4())
        session_store[session_id] = state
```

## 4. üåê Adaptaci√≥n Multiling√ºe

### Desaf√≠o
El proyecto original se enfoca en espa√±ol, pero los modelos soportan m√∫ltiples idiomas.

### Soluci√≥n Arquitect√≥nica
```python
class LanguageConfig:
    LANGUAGES = {
        "es": {
            "content_model": "paraphrase-multilingual-MiniLM-L12-v2",
            "gec_model": "dccuchile/bert-base-spanish-wwm-cased",
            "generator_prompt": "Eres un tutor de espa√±ol..."
        },
        "en": {
            "content_model": "paraphrase-multilingual-MiniLM-L12-v2",
            "gec_model": "bert-base-uncased",
            "generator_prompt": "You are an English tutor..."
        }
    }
```

## 5. üîß Optimizaci√≥n de Recursos

### Memoria
- **Modelos cargados una sola vez**: Patrones Singleton
- **Liberaci√≥n de memoria no utilizada**: `torch.cuda.empty_cache()`
- **Cuantizaci√≥n**: Modelos GGUF para reducir uso de RAM

### CPU
- **Procesamiento paralelo**: `asyncio` para I/O
- **Batch processing**: Procesar m√∫ltiples tokens simult√°neamente
- **Thread pools**: Para operaciones CPU-intensivas

### Disco
- **Modelos comprimidos**: GGUF vs formatos originales
- **Lazy loading**: Descargar modelos solo cuando se necesitan
- **Cache local**: Evitar descargas repetidas

## 6. üêõ Debugging y Monitoreo

### Logging Estructurado
```python
from loguru import logger

logger.info("Processing chat request", extra={
    "user_id": session_id,
    "scenario": scenario,
    "message_length": len(user_message)
})
```

### M√©tricas de Rendimiento
```python
import time

def measure_time(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        duration = time.time() - start
        logger.info(f"{func.__name__} completed in {duration:.2f}s")
        return result
    return wrapper
```

### Health Checks
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "models": {
            "content": content_model is not None,
            "gec": gec_model is not None,
            "generator": generator_model is not None
        },
        "memory": psutil.virtual_memory()._asdict()
    }
```

## 7. üîÆ Mejoras Futuras

### Corto Plazo
- [ ] Implementar modelo GEC fine-tuned real
- [ ] A√±adir soporte para m√°s escenarios
- [ ] Mejorar la UI para mostrar errores gramaticales
- [ ] Optimizar tiempo de respuesta

### Mediano Plazo
- [ ] Soporte para m√∫ltiples idiomas
- [ ] Sistema de persistencia de sesiones
- [ ] M√©tricas de progreso del usuario
- [ ] Integraci√≥n con modelos locales m√°s peque√±os

### Largo Plazo
- [ ] Interfaz de escritorio nativa (Electron/Tauri)
- [ ] Modo offline completo
- [ ] Personalizaci√≥n adaptativa del tutor
- [ ] Exportaci√≥n de progreso y estad√≠sticas

## üìö Referencias T√©cnicas

- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - Bindings Python para llama.cpp
- [Sentence Transformers](https://www.sbert.net/) - Embeddings multiling√ºes
- [FastAPI](https://fastapi.tiangolo.com/) - Backend async Python
- [Docker](https://www.docker.com/) - Contenerizaci√≥n
- [COWS-L2H Dataset](https://github.com/chrisjbryant/errant) - Dataset para GEC en espa√±ol

---

**Nota**: Este documento evolucionar√° con el proyecto. Contribuciones y sugerencias son bienvenidas.
