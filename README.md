# ğŸ—£ï¸ Conversationally Desktop â€“ Tutor de idiomas con IA para Debian

> Basado en **Conversationally**, el proyecto ganador del capstone de UC Berkeley: un tutor conversacional de idiomas con IA.  
> Repositorio original: [team-langbot/conversationally](https://github.com/team-langbot/conversationally)

**Conversationally Desktop** es una adaptaciÃ³n local del aclamado tutor de idiomas conversacional creado por un equipo de la MaestrÃ­a en Ciencia de Datos de UC Berkeley. Esta versiÃ³n reimplementa la arquitectura original (basada en AWS) para que puedas ejecutarla **directamente en tu mÃ¡quina Debian** (o cualquier Linux con Docker), sin depender de la nube, manteniendo toda la potencia de los modelos de NLP.

---

## ğŸ¯ Â¿QuÃ© hace Conversationally?

Conversationally te permite mantener conversaciones en espaÃ±ol (u otros idiomas) con un bot inteligente que:

- **Mantiene el tema** de la conversaciÃ³n para que no te desvÃ­es del objetivo de aprendizaje.
- **Detecta errores gramaticales** de concordancia de gÃ©nero y nÃºmero (y potencialmente otros).
- **Genera pistas (scaffolding)** cuando cometes un error, ayudÃ¡ndote a autocorregirte sin darte la respuesta directamente.

Todo esto ocurre en una interfaz web sencilla, accesible desde tu navegador.

---

## ğŸ§  Arquitectura del sistema

Esta versiÃ³n local replica la arquitectura original utilizando componentes open-source y ejecutÃ¡ndose en tu propio hardware.

```mermaid
graph TD
    A[Frontend React] -->|HTTP| B[Backend FastAPI]
    B --> C[Modelo 1: ClasificaciÃ³n de Contenido<br/>Sentence Transformers]
    B --> D[Modelo 2: CorrecciÃ³n Gramatical GEC<br/>BETO fine-tuned]
    B --> E[Modelo 3: GeneraciÃ³n de Respuestas<br/>Mistral 7B]
    B --> F[GestiÃ³n de DiÃ¡logo]
    F --> G[Respuesta + Feedback]
    G --> A
```

### Componentes principales:

| Componente | TecnologÃ­a | FunciÃ³n |
|------------|------------|---------|
| **Frontend** | React (desde `langbot-ui`) | Interfaz de chat web |
| **Backend** | FastAPI (Python) | Orquesta los modelos y gestiona la lÃ³gica de conversaciÃ³n |
| **Modelo de Contenido** | Sentence Transformers (`paraphrase-multilingual-MiniLM-L12-v2`) | Calcula similitud coseno entre el mensaje del usuario y el tema esperado |
| **Modelo GEC** | BETO (Spanish BERT) fine-tuned con COWS-L2H | Clasifica cada token en: correcto, error de gÃ©nero, error de nÃºmero |
| **Modelo Generador** | Mistral 7B Instruct (cuantizado) | Produce respuestas y pistas contextuales |

---

## ğŸ“ Estructura del repositorio

```
conversationally-desktop/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # Punto de entrada FastAPI
â”‚   â”‚   â”œâ”€â”€ models/                  # Carga e inferencia de modelos
â”‚   â”‚   â”‚   â”œâ”€â”€ content.py            # Sentence Transformers
â”‚   â”‚   â”‚   â”œâ”€â”€ gec.py                # Modelo BETO para GEC
â”‚   â”‚   â”‚   â””â”€â”€ generator.py          # Mistral 7B (vÃ­a transformers o llama.cpp)
â”‚   â”‚   â”œâ”€â”€ routers/                  # Endpoints de la API
â”‚   â”‚   â”‚   â””â”€â”€ chat.py
â”‚   â”‚   â””â”€â”€ schemas/                  # Pydantic models
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ (cÃ³digo fuente de langbot-ui adaptado)
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ models/                            # (Opcional) Pesos descargados localmente
â”‚   â”œâ”€â”€ sentence-transformer/
â”‚   â”œâ”€â”€ beto-gec/
â”‚   â””â”€â”€ mistral-7b-instruct-v0.2.Q4_K_M.gguf
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile                           # Comandos Ãºtiles
â”œâ”€â”€ docs/                               # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ desafios.md                     # ExplicaciÃ³n de retos tÃ©cnicos
â”‚   â””â”€â”€ creditos.md                      # Atribuciones al equipo original
â””â”€â”€ README.md
```

---

## ğŸš€ InstalaciÃ³n y ejecuciÃ³n en Debian

### Requisitos del sistema

- **Sistema operativo**: Debian 11+ / Ubuntu 20.04+ (cualquier Linux con Docker)
- **Memoria RAM**: MÃ­nimo 8 GB (recomendado 16 GB para Mistral 7B)
- **Espacio en disco**: 10 GB (para modelos y dependencias)
- **Docker y Docker Compose** (opcional pero recomendado)
- **Python 3.10+** y **Node.js 18+** (si se instala manualmente)

### OpciÃ³n 1: InstalaciÃ³n con Docker (recomendada)

```bash
# 1. Clonar el repositorio
git clone https://github.com/tu-usuario/conversationally-desktop.git
cd conversationally-desktop

# 2. Configurar variables de entorno (opcional)
cp backend/.env.example backend/.env

# 3. Descargar los modelos (automÃ¡tico al construir, pero puedes pre-descargarlos)
make download-models

# 4. Construir y levantar con Docker Compose
docker-compose up --build
```

Una vez que los contenedores estÃ©n en ejecuciÃ³n, abre tu navegador en `http://localhost:3000` (o el puerto configurado).

### OpciÃ³n 2: InstalaciÃ³n manual (para desarrollo)

#### Backend (FastAPI)

```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Descargar modelos (ajusta las rutas en .env)
python scripts/download_models.py

# Ejecutar servidor
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Frontend (React)

```bash
cd frontend
npm install
npm start
```

Accede a `http://localhost:3000`.

---

## ğŸ§ª Uso de la aplicaciÃ³n

1. Abre la interfaz web.
2. Selecciona un escenario de conversaciÃ³n (por ejemplo: "En el restaurante", "Presentarte", etc.).
3. Comienza a escribir en espaÃ±ol. El bot te responderÃ¡ y, si cometes un error gramatical, te darÃ¡ una pista.
4. Puedes ver el anÃ¡lisis de "on-topic" y las correcciones token a token en la consola de desarrollador (o en futuras versiones en la UI).

---

## ğŸ§— DesafÃ­os y consideraciones tÃ©cnicas

Este proyecto no es un simple "copiar y pegar". Requiere entender y resolver varios retos:

### 1. **Modelos pesados en local**
   - Mistral 7B necesita al menos 8 GB de RAM en versiÃ³n cuantizada (GGUF). Para CPU, se recomienda usar [llama.cpp](https://github.com/ggerganov/llama.cpp) con bindings en Python (`llama-cpp-python`). En el backend hemos incluido soporte para cargar modelos GGUF.
   - El modelo GEC (BETO fine-tuned) no estÃ¡ publicado en Hugging Face. Debes entrenarlo tÃº mismo siguiendo los notebooks originales o contactar a los autores. Como alternativa, hemos incluido un script para fine-tunearlo con el dataset COWS-L2H (ver `backend/scripts/train_gec.py`).

### 2. **OrquestaciÃ³n de tres modelos en tiempo real**
   - La latencia total puede ser alta si no se optimiza. Se recomienda usar asyncio y cargar los modelos en memoria una sola vez.
   - Implementamos cachÃ© de respuestas para escenarios comunes.

### 3. **Mantenimiento del estado conversacional**
   - El sistema debe recordar el contexto de la conversaciÃ³n. Usamos una lista de mensajes que se pasa al generador (similar a los `messages` de ChatGPT).

### 4. **Idioma**
   - El proyecto original se centra en espaÃ±ol, pero los modelos de Sentence Transformers y Mistral son multilingÃ¼es. Puedes adaptarlo a otros idiomas cambiando el dataset de GEC y ajustando prompts.

---

## ğŸ¤ Contribuciones

Â¿Quieres mejorar esta adaptaciÃ³n? Â¡Genial! Las Ã¡reas donde mÃ¡s ayuda se necesita:

- **Modelo GEC**: Entrenar y subir una versiÃ³n lista para usar del modelo BETO fine-tuned.
- **Frontend**: Mejorar la UI para mostrar los errores token a token y las pistas de forma mÃ¡s amigable.
- **OptimizaciÃ³n**: Reducir el consumo de memoria y acelerar la inferencia.
- **DocumentaciÃ³n**: Traducir este README a inglÃ©s y crear tutoriales en vÃ­deo.

Por favor, abre un issue o un pull request. Revisa `docs/desafios.md` para mÃ¡s detalles tÃ©cnicos.

---

## ğŸ“œ CrÃ©ditos y licencia

Este proyecto es una adaptaciÃ³n del trabajo original de:

- **Aastha Khanna**
- **Isabel Chan**
- **Jess Matthews**
- **Mon Young**
- **Ram Senthamarai**

y su repositorio: [team-langbot/conversationally](https://github.com/team-langbot/conversationally)

El cÃ³digo nuevo de esta adaptaciÃ³n estÃ¡ bajo licencia MIT (ver `LICENSE`). Los modelos y datasets utilizados tienen sus propias licencias (consulta la documentaciÃ³n de cada uno).

**Agradecimiento especial** al equipo de UC Berkeley por compartir su increÃ­ble proyecto y hacer posible esta versiÃ³n local.

---

## ğŸ“¬ Contacto

Si tienes preguntas o sugerencias, abre un issue en GitHub o contacta al mantenedor: [tu-email@ejemplo.com]

---

**Â¡Disfruta aprendiendo idiomas con IA en tu propia mÃ¡quina!** ğŸš€
